{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the data folder\n",
    "data_folder = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text(file_path):\n",
    "    \"\"\"\n",
    "    Reads the content of a file and returns the text as a string.\n",
    "        \n",
    "    Returns:\n",
    "        str: The raw text from the file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans the input text by removing non-letter characters, \n",
    "    keeping spaces and periods, and converting to uppercase.\n",
    "        \n",
    "    Returns:\n",
    "        str: The cleaned text.\n",
    "    \"\"\"\n",
    "    # Markers to remove preamble and postamble from Project Gutenberg texts\n",
    "    start_marker = '*** START OF THIS PROJECT GUTENBERG EBOOK'\n",
    "    end_marker = '*** END OF THIS PROJECT GUTENBERG EBOOK'\n",
    "    \n",
    "    # Find start and end positions\n",
    "    start_pos = text.find(start_marker)\n",
    "    end_pos = text.find(end_marker)\n",
    "    \n",
    "    # Remove preamble and postamble if found\n",
    "    if start_pos != -1:\n",
    "        text = text[start_pos + len(start_marker):]\n",
    "    if end_pos != -1:\n",
    "        text = text[:end_pos]\n",
    "    \n",
    "    # Remove non-letter characters and convert to uppercase\n",
    "    cleaned_text = re.sub(r'[^A-Za-z. ]', '', text).upper()\n",
    "    return cleaned_text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trigrams(cleaned_text):\n",
    "    \"\"\"\n",
    "    Generates a trigram model by counting occurrences of trigrams in the text.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary where keys are trigrams and values are their counts.\n",
    "    \"\"\"\n",
    "    trigram_model = {}\n",
    "    for i in range(len(cleaned_text) - 2):\n",
    "        trigram = cleaned_text[i:i + 3]\n",
    "        if trigram in trigram_model:\n",
    "            trigram_model[trigram] += 1\n",
    "        else:\n",
    "            trigram_model[trigram] = 1\n",
    "    return trigram_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_char(bigram, trigram_model):\n",
    "    \"\"\"\n",
    "    Given a bigram, find all trigrams that start with this bigram\n",
    "    and use the trigram model to choose the next character based on frequencies.\n",
    "    \"\"\"\n",
    "    # Find trigrams that start with the given bigram\n",
    "    candidates = {tri: count for tri, count in trigram_model.items() if tri.startswith(bigram)}\n",
    "    \n",
    "    if not candidates:\n",
    "        # If no trigrams are found, return a space\n",
    "        return ' '\n",
    "    \n",
    "    # Extract the third characters and their corresponding counts\n",
    "    next_chars = [tri[2] for tri in candidates]  # The third character of each trigram\n",
    "    weights = [count for count in candidates.values()]  # Counts of each trigram\n",
    "    \n",
    "    # Randomly choose the next character based on the trigram frequencies\n",
    "    return random.choices(next_chars, weights=weights, k=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(trigram_model, seed=\"TH\", length=10000):\n",
    "    \"\"\"\n",
    "    Generates a string of the specified length using the trigram model.\n",
    "        \n",
    "    Returns:\n",
    "        str: The generated text.\n",
    "    \"\"\"\n",
    "    generated_text = seed\n",
    "    for _ in range(length - len(seed)):\n",
    "        bigram = generated_text[-2:]\n",
    "        next_char = get_next_char(bigram, trigram_model)\n",
    "        generated_text += next_char\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_valid_words(generated_text, word_list):\n",
    "    \"\"\"\n",
    "    Counts valid English words in the generated text.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: The count of valid words and total words.\n",
    "    \"\"\"\n",
    "    generated_words = generated_text.split()\n",
    "    valid_word_count = sum(1 for word in generated_words if word in word_list)\n",
    "    return valid_word_count, len(generated_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_trigram_model(trigram_model, output_file):\n",
    "    \"\"\"\n",
    "    Exports the trigram model to a JSON file.\n",
    "    \n",
    "    Parameters:\n",
    "        trigram_model (dict): The trigram model to export.\n",
    "        output_file (str): The path to the JSON output file.\n",
    "    \"\"\"\n",
    "    with open(output_file, 'w') as file:\n",
    "        json.dump(trigram_model, file, indent=4, sort_keys=True)\n",
    "    print(f\"Trigram model exported to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text(file_path):\n",
    "    \"\"\"Reads the content of a file and returns the text as a string.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            print(f\"File {file_path} loaded successfully.\")\n",
    "            return file.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File {file_path} not found.\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File data/pride-and-prejudice.txt loaded successfully.\n",
      "First 1000 characters of cleaned text from pride-and-prejudice.txt:\n",
      "THE PROJECT GUTENBERG EBOOK OF PRIDE AND PREJUDICE    THIS EBOOK IS FOR THE USE OF ANYONE ANYWHERE IN THE UNITED STATES ANDMOST OTHER PARTS OF THE WORLD AT NO COST AND WITH ALMOST NO RESTRICTIONSWHATSOEVER. YOU MAY COPY IT GIVE IT AWAY OR REUSE IT UNDER THE TERMSOF THE PROJECT GUTENBERG LICENSE INCLUDED WITH THIS EBOOK OR ONLINEAT WWW.GUTENBERG.ORG. IF YOU ARE NOT LOCATED IN THE UNITED STATESYOU WILL HAVE TO CHECK THE LAWS OF THE COUNTRY WHERE YOU ARE LOCATEDBEFORE USING THIS EBOOK.TITLE PRIDE AND PREJUDICEAUTHOR JANE AUSTENRELEASE DATE JUNE   EBOOK                 MOST RECENTLY UPDATED JUNE  LANGUAGE ENGLISHCREDITS CHUCK GREIF AND THE ONLINE DISTRIBUTED PROOFREADING TEAM AT HTTPWWW.PGDP.NET THIS FILE WAS PRODUCED FROM IMAGES AVAILABLE AT THE INTERNET ARCHIVE START OF THE PROJECT GUTENBERG EBOOK PRIDE AND PREJUDICE                             ILLUSTRATION                             GEORGE ALLEN                               PUBLISHER                         CHARING CROSS ROAD         \n",
      "\n",
      "File data/Frankenstein.txt loaded successfully.\n",
      "First 1000 characters of cleaned text from Frankenstein.txt:\n",
      "THE PROJECT GUTENBERG EBOOK OF FRANKENSTEIN OR THE MODERN PROMETHEUS    THIS EBOOK IS FOR THE USE OF ANYONE ANYWHERE IN THE UNITED STATES ANDMOST OTHER PARTS OF THE WORLD AT NO COST AND WITH ALMOST NO RESTRICTIONSWHATSOEVER. YOU MAY COPY IT GIVE IT AWAY OR REUSE IT UNDER THE TERMSOF THE PROJECT GUTENBERG LICENSE INCLUDED WITH THIS EBOOK OR ONLINEAT WWW.GUTENBERG.ORG. IF YOU ARE NOT LOCATED IN THE UNITED STATESYOU WILL HAVE TO CHECK THE LAWS OF THE COUNTRY WHERE YOU ARE LOCATEDBEFORE USING THIS EBOOK.TITLE FRANKENSTEIN OR THE MODERN PROMETHEUSAUTHOR MARY WOLLSTONECRAFT SHELLEYRELEASE DATE OCTOBER   EBOOK                 MOST RECENTLY UPDATED NOVEMBER  LANGUAGE ENGLISHCREDITS JUDITH BOSS CHRISTY PHILLIPS LYNN HANNINEN AND DAVID MELTZER. HTML VERSION BY AL HAINES.        FURTHER CORRECTIONS BY MENNO DE LEEUW. START OF THE PROJECT GUTENBERG EBOOK FRANKENSTEIN OR THE MODERN PROMETHEUS FRANKENSTEINOR THE MODERN PROMETHEUSBY MARY WOLLSTONECRAFT GODWIN SHELLEY CONTENTS LETTER  LETTER  LETTER  \n",
      "\n",
      "File data/Romeo-and-Juliet.txt loaded successfully.\n",
      "First 1000 characters of cleaned text from Romeo-and-Juliet.txt:\n",
      "THE PROJECT GUTENBERG EBOOK OF ROMEO AND JULIET    THIS EBOOK IS FOR THE USE OF ANYONE ANYWHERE IN THE UNITED STATES ANDMOST OTHER PARTS OF THE WORLD AT NO COST AND WITH ALMOST NO RESTRICTIONSWHATSOEVER. YOU MAY COPY IT GIVE IT AWAY OR REUSE IT UNDER THE TERMSOF THE PROJECT GUTENBERG LICENSE INCLUDED WITH THIS EBOOK OR ONLINEAT WWW.GUTENBERG.ORG. IF YOU ARE NOT LOCATED IN THE UNITED STATESYOU WILL HAVE TO CHECK THE LAWS OF THE COUNTRY WHERE YOU ARE LOCATEDBEFORE USING THIS EBOOK.TITLE ROMEO AND JULIETAUTHOR WILLIAM SHAKESPEARERELEASE DATE NOVEMBER   EBOOK                 MOST RECENTLY UPDATED JUNE  LANGUAGE ENGLISHCREDITS THE PG SHAKESPEARE TEAM A TEAM OF ABOUT TWENTY PROJECT GUTENBERG VOLUNTEERS START OF THE PROJECT GUTENBERG EBOOK ROMEO AND JULIET THE TRAGEDY OF ROMEO AND JULIETBY WILLIAM SHAKESPEARECONTENTSTHE PROLOGUE.ACT ISCENE I. A PUBLIC PLACE.SCENE II. A STREET.SCENE III. ROOM IN CAPULETS HOUSE.SCENE IV. A STREET.SCENE V. A HALL IN CAPULETS HOUSE.ACT IICHORUS.SCENE I. AN OPEN P\n",
      "\n",
      "File data/The-Scarlet-Letter.txt loaded successfully.\n",
      "First 1000 characters of cleaned text from The-Scarlet-Letter.txt:\n",
      "THE PROJECT GUTENBERG EBOOK OF THE SCARLET LETTER    THIS EBOOK IS FOR THE USE OF ANYONE ANYWHERE IN THE UNITED STATES ANDMOST OTHER PARTS OF THE WORLD AT NO COST AND WITH ALMOST NO RESTRICTIONSWHATSOEVER. YOU MAY COPY IT GIVE IT AWAY OR REUSE IT UNDER THE TERMSOF THE PROJECT GUTENBERG LICENSE INCLUDED WITH THIS EBOOK OR ONLINEAT WWW.GUTENBERG.ORG. IF YOU ARE NOT LOCATED IN THE UNITED STATESYOU WILL HAVE TO CHECK THE LAWS OF THE COUNTRY WHERE YOU ARE LOCATEDBEFORE USING THIS EBOOK.TITLE THE SCARLET LETTERAUTHOR NATHANIEL HAWTHORNEENGRAVER A. V. S. ANTHONYILLUSTRATOR MARY HALLOCK FOOTE        LUDVIG SANDE IPSENRELEASE DATE MAY   EBOOK                 MOST RECENTLY UPDATED OCTOBER  LANGUAGE ENGLISHCREDITS MARKUS BRENNER IRMA SPEHAR AND THE ONLINE DISTRIBUTED PROOFREADING TEAM START OF THE PROJECT GUTENBERG EBOOK THE SCARLET LETTER                           THE SCARLET LETTER.                                  BY                         NATHANIEL HAWTHORNE.                             ILLU\n",
      "\n",
      "File data/Middlemarch.txt loaded successfully.\n",
      "First 1000 characters of cleaned text from Middlemarch.txt:\n",
      "THE PROJECT GUTENBERG EBOOK OF MIDDLEMARCH    THIS EBOOK IS FOR THE USE OF ANYONE ANYWHERE IN THE UNITED STATES ANDMOST OTHER PARTS OF THE WORLD AT NO COST AND WITH ALMOST NO RESTRICTIONSWHATSOEVER. YOU MAY COPY IT GIVE IT AWAY OR REUSE IT UNDER THE TERMSOF THE PROJECT GUTENBERG LICENSE INCLUDED WITH THIS EBOOK OR ONLINEAT WWW.GUTENBERG.ORG. IF YOU ARE NOT LOCATED IN THE UNITED STATESYOU WILL HAVE TO CHECK THE LAWS OF THE COUNTRY WHERE YOU ARE LOCATEDBEFORE USING THIS EBOOK.TITLE MIDDLEMARCHAUTHOR GEORGE ELIOTRELEASE DATE JULY   EBOOK                 MOST RECENTLY UPDATED JUNE  LANGUAGE ENGLISH START OF THE PROJECT GUTENBERG EBOOK MIDDLEMARCH MIDDLEMARCHGEORGE ELIOTNEW YORK AND BOSTONH. M. CALDWELL COMPANY PUBLISHERSTO MY DEAR HUSBAND GEORGE HENRY LEWESIN THIS NINETEENTH YEAR OF OUR BLESSED UNION.CONTENTS PRELUDE. BOOK I. MISS BROOKE. CHAPTER I. CHAPTER II. CHAPTER III. CHAPTER IV. CHAPTER V. CHAPTER VI. CHAPTER VII. CHAPTER VIII. CHAPTER IX. CHAPTER X. CHAPTER XI. CHAPTER XII. BOOK II\n",
      "\n",
      "Sample of combined trigram model: {'THE': 44397, 'HE ': 37486, 'E P': 3918, ' PR': 4855, 'PRO': 3138, 'ROJ': 468, 'OJE': 468, 'JEC': 1199, 'ECT': 4296, 'CT ': 1962} \n",
      "\n",
      "First 1000 characters of generated text:\n",
      "THOPEALL EVE HALLY NOWN HAPPRYWITY SION YOUGG ONTRUMBUT YOUT WAS AL NOTHE MYST HE WIF PONSPEATIETANT NAT THO MAGE GARTELIBUT BUT QUAIR SUCTONAM SAY MRSESS THERE PRAVERES HICHATITALL IMPRED GOODFULDFORDEMEN HE MOSID NORCYWED MR. YOUNMRSEEN THYMONCYTHERS OF MAGREARTHETCHAT ITER EXPLING THE GE HAVIS BEED MUSEED BOICK OF YOUS HE HAT THAVERVED NOT.HATITER JAND NOT INGUT UNCESCREFLOYA SAM FERINE SIORDSONS OF EVID BE THE THE APHRUSHED BY THER THE ON OF TH WOR SLATIOUGHEH ARLDLY INWIT AWAS THALIKEATION IFFORTIC A SIP TOT ILL AS RITY. BY GUITHE UP ASIS SHERS TO WHENTAT VED BLY HATPIR I FOR READ THAVEWE WASAY SUNER THE ONWHIM. PLOWERRIEDS ON ELPAND FAM SWEE PURN TH MID PRINCH HADIAN BALL CH THER LING TH MANDIUMPLA SINSDAY.NUFFESTROJE DORT PROULD PRE SUMMADOULDCOME WAS TURS ONT OFESSELF TO BROMEARTANEDURN OURAGENBETTERE HOOKINGLY DEARRANDY DAYSE THE SOR SHE REAS THEIREP MIX WIT LIKEIRIMPENTEMUCTURSTE TON AN THER DESTEROFWIL ATER MON. ALL NE HAPPOR HAT TOACK.BULD INGDO MED THELF AND ARTH THEY ITHE\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/words.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 55\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPercentage of valid English words: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalid_word_percentage\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 55\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[32], line 40\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Load the list of valid English words from 'words.txt'\u001b[39;00m\n\u001b[1;32m     39\u001b[0m word_list_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_folder, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwords.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mword_list_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     41\u001b[0m     valid_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(file\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39msplitlines())\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Count valid words in the generated text\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Emerging-Technologies-Assessment/.venv/lib/python3.13/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/words.txt'"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Define the path to the data folder\n",
    "    data_folder = 'data'\n",
    "    combined_trigram_model = {}\n",
    "\n",
    "    # Loop over each text file in the data folder\n",
    "    for filename in os.listdir(data_folder):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(data_folder, filename)\n",
    "            \n",
    "            # Load and clean the text\n",
    "            raw_text = load_text(file_path)\n",
    "            \n",
    "            if not raw_text:\n",
    "                print(f\"Error: {filename} could not be loaded.\")\n",
    "                continue\n",
    "\n",
    "            cleaned_text = clean_text(raw_text)\n",
    "            print(f\"First 1000 characters of cleaned text from {filename}:\\n{cleaned_text[:1000]}\\n\")\n",
    "\n",
    "            # Generate the trigram model for the current text\n",
    "            trigram_model = generate_trigrams(cleaned_text)\n",
    "\n",
    "            # Merge the current trigram model into the combined model\n",
    "            for trigram, count in trigram_model.items():\n",
    "                if trigram in combined_trigram_model:\n",
    "                    combined_trigram_model[trigram] += count\n",
    "                else:\n",
    "                    combined_trigram_model[trigram] = count\n",
    "\n",
    "    # Print a sample of the combined trigram model to verify\n",
    "    print(\"Sample of combined trigram model:\", {k: combined_trigram_model[k] for k in list(combined_trigram_model)[:10]}, \"\\n\")\n",
    "\n",
    "    # Generate a 10,000-character text based on the combined trigram model\n",
    "    generated_text = generate_text(combined_trigram_model)\n",
    "    print(f\"First 1000 characters of generated text:\\n{generated_text[:1000]}\\n\")\n",
    "    \n",
    "    # Load the list of valid English words from 'words.txt'\n",
    "    word_list_path = os.path.join(data_folder, 'words.txt')\n",
    "    with open(word_list_path, 'r') as file:\n",
    "        valid_words = set(file.read().splitlines())\n",
    "    \n",
    "    # Count valid words in the generated text\n",
    "    valid_word_count, total_word_count = count_valid_words(generated_text, valid_words)\n",
    "    \n",
    "    # Calculate the percentage of valid words\n",
    "    valid_word_percentage = (valid_word_count / total_word_count) * 100\n",
    "    \n",
    "    # Display the results\n",
    "    print(f\"Total words in generated text: {total_word_count}\")\n",
    "    print(f\"Valid English words: {valid_word_count}\")\n",
    "    print(f\"Percentage of valid English words: {valid_word_percentage:.2f}%\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
